# ğŸ› ï¸ Training Process

Originally, I attempted to run training locally, but since I was using a **Mac** (which lacks NVIDIA GPU and CUDA support), I wasnâ€™t able to run **Stable Diffusion training or DreamBooth locally**.  

To overcome this:  
âœ… I used **Google Colab** â˜ï¸ to perform the training and fine-tuning in the cloud.  
âœ… After training, I transferred the **trained model** to my **local NVIDIA-based PC** ğŸ–¥ï¸ and ran it using **Automatic1111 Web UI** for optimized local image generation.

---

## âš ï¸ Hardware Requirements

ğŸš« **Important Note:**  
- **Mac systems (Intel or M1/M2/M3)** are not compatible for local Stable Diffusion training or for running **Automatic1111 Web UI**.  
- Requires a system with an **NVIDIA GPU** (CUDA support) to run both training and optimized local inference.  
- In this project, I used:  
  - **Google Colab** â˜ï¸ for training  
  - **NVIDIA GPU PC** ğŸ–¥ï¸ for local inference with **Automatic1111 Web UI**

---

## ğŸš€ How DreamBooth Works

DreamBooth fine-tunes an existing model (such as **Stable Diffusion 1.5 or 3.5**) by:

1ï¸âƒ£ **Associating a Unique Token** ğŸ·ï¸  
ğŸ‘‰ You assign a new identifier (token) that doesn't already exist in the model.  
ğŸ‘‰ Example: `"sks_pixarstyle"` â€” this token gets embedded in the model and is used in prompts after training.  

2ï¸âƒ£ **Teaching the Model New Visual Features** ğŸ–Œï¸  
ğŸ‘‰ You provide a small image dataset (e.g. Pixar-style artwork).  
ğŸ‘‰ The model learns **artistic traits** like brush strokes, colors, lighting, and composition.  

3ï¸âƒ£ **Blending New Knowledge Into Stable Diffusion** ğŸ”„  
ğŸ‘‰ DreamBooth **augments** the model â€” it doesn't replace existing knowledge.  
ğŸ‘‰ After training, you can use your token (ex: `"sks_pixarstyle"`) to apply the style to any prompt.

4ï¸âƒ£ **Training Parameters** âš™ï¸  
ğŸ‘‰ DreamBooth fine-tunes **specific layers** of the model â€” not the entire model â€” to avoid overfitting and preserve general capabilities.

---

# âš™ï¸ Training Parameters â€” Explained

### ğŸ–¥ï¸ Settings

**1ï¸âƒ£ Model**  
- Selects your training model.  
- The output is an **incremental checkpoint** of your fine-tuning progress (not a full `.ckpt` until exported).  

---

**2ï¸âƒ£ Concepts**  
Defines what your model is learning â€” helps it associate the **new style** (Pixar-inspired) with your token:

- **Instance Images** ğŸ–¼ï¸ â†’ Path to your training image folder.  
- **Prompt** âœï¸ â†’ Describes what your images represent (ex: `"a Pixar-style pet"`).  
- **Instance Token** ğŸ·ï¸ â†’ The **keyword** that activates your trained style (ex: `"sks_pixarstyle"`).  
- **Class Token** ğŸ·ï¸ â†’ Optional â€” used for training **specific objects or people** (prevents overfitting).  

ğŸš¨ **Concept Boxes** (Concept 1â€“4) â†’ Allow you to train **multiple styles** in one session (ex: Pixar, Watercolor, Anime).  

---

**3ï¸âƒ£ Class Images** ğŸ• â†’  
- Used when training **specific objects** (ex: dog breeds).  
- Helps the model **differentiate between object features** and style.  
- Useful for **object-focused training** â€” not needed for pure stylization.

---

**4ï¸âƒ£ Sample Images** ğŸ§ª â†’  
- Automatically generated **during training** to monitor progress.  
- Helps check for:  
    - **Overfitting** ğŸ”¥ (too much memorization)  
    - **Underfitting** ğŸ§Š (not learning enough)  
- Lets you evaluate model performance in **real-time**.

---

**5ï¸âƒ£ Parameters** âš™ï¸ â†’  
Control how the model learns:  
âœ… Learning rate  
âœ… Training steps  
âœ… Scheduler  
âœ… Regularization  

Proper tuning ensures your model **learns the target style** (Pixar) without overfitting.